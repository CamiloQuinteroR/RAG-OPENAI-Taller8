
# üß† RAG con LangChain, OpenAI y Pinecone

Este proyecto implementa un **sistema de Recuperaci√≥n Aumentada por Generaci√≥n (RAG)** utilizando **LangChain**, **OpenAI GPT** y **Pinecone**.  
El sistema permite responder preguntas fundamentadas en el contenido de documentos o p√°ginas web mediante la integraci√≥n de embeddings, recuperaci√≥n sem√°ntica y generaci√≥n de lenguaje natural.

---

## ‚öôÔ∏è Arquitectura del Proyecto

La arquitectura del sistema RAG est√° compuesta por los siguientes m√≥dulos:

1. **Carga de documentos:**  
   Se obtienen los textos desde URLs o archivos locales y se limpian usando `BeautifulSoup`.

2. **Divisi√≥n de texto (Chunking):**  
   Se fragmenta el texto con `RecursiveCharacterTextSplitter` para optimizar la recuperaci√≥n sem√°ntica.

3. **Generaci√≥n de embeddings:**  
   Cada fragmento se convierte en un vector mediante `OpenAIEmbeddings`.

4. **Almacenamiento vectorial:**  
   Los embeddings se indexan en **Pinecone**, que permite realizar b√∫squedas por similitud.

5. **Recuperaci√≥n y construcci√≥n de contexto:**  
   LangChain busca los fragmentos m√°s relevantes con respecto a la consulta del usuario.

6. **Generaci√≥n de respuesta (RAG):**  
   Se utiliza el modelo **GPT-4-turbo** de OpenAI para generar una respuesta enriquecida con el contexto recuperado.

---

## üß≠ Diagrama de Arquitectura



---

## üß© Componentes Principales

| Componente                         | Descripci√≥n                                                         |
| ---------------------------------- | ------------------------------------------------------------------- |
| **LangChain**                      | Framework principal para construir el flujo RAG.                    |
| **OpenAI GPT-4-turbo**             | Modelo de lenguaje que genera respuestas fundamentadas en contexto. |
| **OpenAIEmbeddings**               | Genera representaciones vectoriales de los fragmentos de texto.     |
| **Pinecone**                       | Base de datos vectorial para b√∫squedas sem√°nticas.                  |
| **BeautifulSoup (bs4)**            | Limpieza y extracci√≥n de texto desde HTML.                          |
| **RecursiveCharacterTextSplitter** | Divide el texto en fragmentos para el embedding.                    |

---

## üß∞ Instalaci√≥n y Configuraci√≥n

### 1Ô∏è‚É£ Clonar el repositorio

```bash
git clone https://github.com/tu-usuario/Taller-LangChain-LLM.git
cd Taller-LangChain-LLM
```

---

### 2Ô∏è‚É£ Crear entorno virtual (opcional pero recomendado)

```bash
python -m venv .venv
.venv\Scripts\activate   # En Windows
# source .venv/bin/activate   # En macOS o Linux
```

---

### 3Ô∏è‚É£ Instalar dependencias

```bash
pip install langchain openai pinecone-client langchain-community langchain-text-splitters beautifulsoup4
```

---

### 4Ô∏è‚É£ Configurar variables de entorno

Creamos un archivo `.env` en la ra√≠z del proyecto con tus claves:

```bash
OPENAI_API_KEY=""
PINECONE_API_KEY=""
PINECONE_ENVIRONMENT=""
```


---

## üöÄ Ejecuci√≥n del Proyecto

Tenemos tres opciones:

1. El proyecto se ejecuta directamente en el notebook `RAG-OPENAI-PINECONE-ipynb`.

### Pasos dentro del notebook:

1. **Instalar dependencias** (primera celda).
2. **Configurar las API keys** (segunda celda).
3. **Cargar el documento** desde una URL (ejemplo: blog de Lilian Weng).
4. **Generar embeddings** e indexarlos en Pinecone.
5. **Consultar el sistema** con una pregunta en lenguaje natural.

---
2. Podemos ejecutar el proyecto con los archivos .py, para esto seguiremos los siguientes comandos:

```bash
python src/app.py
```
Esto desde la raiz del proyecto.

---

## üß™ Ejemplo de Ejecuci√≥n

1. Para el proyecto ejecutando desde cosola app.py:

**Consulta:**

![alt text](images/image.png)

```text
Que es la ia?
```

**Respuesta generada:**

![alt text](images/image2.png)
```text
La inteligencia artificial (IA) consiste en ense√±ar a las m√°quinas a realizar tareas que normalmente requieren inteligencia humana, como aprender, adaptarse y crear. Esto incluye comprender el lenguaje, analizar datos y generar sugerencias √∫tiles. La IA combina diversas disciplinas, como inform√°tica, an√°lisis de datos, estad√≠stica, neurociencia, ling√º√≠stica y filosof√≠a. Un ejemplo de IA es el reconocimiento √≥ptico de caracteres (OCR), que convierte texto en im√°genes en datos estructurados, facilitando la obtenci√≥n de informaci√≥n valiosa.
```
2. Para el notebook:

**Consulta:**

![alt text](images/image3.png)

```text  
What is task decomposition?
```

**Respuesta generada:**

![alt text](images/image-1.png)

```
--- Respuesta del agente RAG ---

Task decomposition is the process of breaking down a larger task into smaller, more manageable sub-tasks or goals. This can be achieved in several ways:

1. Using large language models (LLMs) with simple prompts, such as asking for "Steps for XYZ" or "What are the subgoals for achieving XYZ?"
2. Employing task-specific instructions, like "Write a story outline" for writing a novel.
3. Incorporating human inputs to guide the decomposition process.

Additionally, there is a distinct approach called LLM+P, which involves using an external classical planner for long-horizon planning. This method utilizes the Planning Domain Definition Language (PDDL) to describe the planning problem, where the LLM translates the problem into PDDL, requests a classical planner to generate a plan, and then translates that plan back into natural language.
Si quieres probar ask() ahora, descomenta la llamada o ejecuta ask("tu pregunta").             
```

---

## üñºÔ∏è Captura de Ejemplo y explicaci√≥n

Al ejecutar nuestro archivo app.py, podemos ver que se cargan los documentos de referencia, o bien el contenido de cualquier pagina web que le proporcionemos a nuestro programa, en el caos de cargar un docuemento, en este caso Referencias.txt podremos ver la siguiente salida en consola cuando el docuemento es porcesado:

![alt text](images/1.png)

En esta ejecuci√≥n, el sistema carg√≥ un archivo local desde el equipo y lo proces√≥ como fuente de conocimiento. El pipeline de RAG ley√≥ el documento, lo dividi√≥ en 10 fragmentos (chunks) y posteriormente gener√≥ los embeddings correspondientes para almacenarlos en la base vectorial Pinecone. Este resultado demuestra que el flujo completo ‚Äîdesde la carga hasta la indexaci√≥n‚Äî funciona correctamente con documentos locales de tama√±o reducido.

Mientras que cuando se carga una pagina web como por ejemplo https://lilianweng.github.io/posts/2023-06-23-agent/ vemos lo siguiente:

![alt text](images/5.png)

En este caso, el sistema utiliz√≥ el cargador web para extraer el contenido de una p√°gina en l√≠nea, proces√°ndolo de la misma forma que un documento local. Debido a que la p√°gina conten√≠a m√°s informaci√≥n, el texto se dividi√≥ en 63 fragmentos, los cuales fueron convertidos en vectores e indexados en Pinecone. Esta ejecuci√≥n evidencia que el agente RAG puede integrar y comprender fuentes de datos externas, permitiendo consultas sobre informaci√≥n proveniente directamente de la web.

En los dos casos, el rag funciona correctamente y podremos realizarle preguntas como las siguientes:

![alt text](images/2.png)

![alt text](images/3.png)

![alt text](images/4.png)

En este caso el agente respondio teniendo en cuenta la pagina web proporcionada de referencia, como podemos ver, funciona de forma correcta. 

---

## üë§ Autor

**Camilo Andr√©s Quintero Rodr√≠guez**
Proyecto: *Creaci√≥n de un agente RAG con LangChain, OpenAI y Pinecone*
Escuela Colombiana de Ingenier√≠a Julio Garavito ‚Äì 2025

