
# üß† RAG con LangChain, OpenAI y Pinecone

Este proyecto implementa un **sistema de Recuperaci√≥n Aumentada por Generaci√≥n (RAG)** utilizando **LangChain**, **OpenAI GPT** y **Pinecone**.  
El sistema permite responder preguntas fundamentadas en el contenido de documentos o p√°ginas web mediante la integraci√≥n de embeddings, recuperaci√≥n sem√°ntica y generaci√≥n de lenguaje natural.

En este reposositorio se encuentra un proyeco organizado en varios archivos .py, cada uno de estos archivos representa una parte fundamental para el desarollo de nuestro RAG como veremos mas adeltante, as√≠ mismo, podemos encontrar dos archivos jupyter, el primer archivo relevante es Guia.ipynb, este archivo contiene la explicaci√≥n del tutorial que se encuentra en este link https://python.langchain.com/docs/tutorials/llm_chain/, all√≠ logramos familiarizarnos con los conceptos b√°icos de LangChain siguiendo el tutorial, esto nos proporcion√≥ los conocimientos necesarios para desarrollar neustro agente. El segundo archivo, RAG-OPENAI-PINECONE.ipynb, contiene la misma aplicaci√≥n de nuestro proyecto pero esta vez, organizado y explicado en el notebook para entender el flujo del agente RAG usando OpenAI y Pinecone. 

---

## ‚öôÔ∏è Arquitectura del Proyecto

La arquitectura del sistema RAG est√° compuesta por los siguientes m√≥dulos:

1. **Carga de documentos:**  
   Se obtienen los textos desde URLs o archivos locales y se limpian usando `BeautifulSoup`.

2. **Divisi√≥n de texto (Chunking):**  
   Se fragmenta el texto con `RecursiveCharacterTextSplitter` para optimizar la recuperaci√≥n sem√°ntica.

3. **Generaci√≥n de embeddings:**  
   Cada fragmento se convierte en un vector mediante `OpenAIEmbeddings`.

4. **Almacenamiento vectorial:**  
   Los embeddings se indexan en **Pinecone**, que permite realizar b√∫squedas por similitud.

5. **Recuperaci√≥n y construcci√≥n de contexto:**  
   LangChain busca los fragmentos m√°s relevantes con respecto a la consulta del usuario.

6. **Generaci√≥n de respuesta (RAG):**  
   Se utiliza el modelo **GPT-4-turbo** de OpenAI para generar una respuesta enriquecida con el contexto recuperado.


---

## üß© Componentes Principales

| Componente                         | Descripci√≥n                                                         |
| ---------------------------------- | ------------------------------------------------------------------- |
| **LangChain**                      | Framework principal para construir el flujo RAG.                    |
| **OpenAI GPT-4-turbo**             | Modelo de lenguaje que genera respuestas fundamentadas en contexto. |
| **OpenAIEmbeddings**               | Genera representaciones vectoriales de los fragmentos de texto.     |
| **Pinecone**                       | Base de datos vectorial para b√∫squedas sem√°nticas.                  |
| **BeautifulSoup (bs4)**            | Limpieza y extracci√≥n de texto desde HTML.                          |
| **RecursiveCharacterTextSplitter** | Divide el texto en fragmentos para el embedding.                    |

---

## üß∞ Instalaci√≥n y Configuraci√≥n

### 1Ô∏è‚É£ Clonar el repositorio

```bash
git clone https://github.com/tu-usuario/Taller-LangChain-LLM.git
cd Taller-LangChain-LLM
```

---

### 2Ô∏è‚É£ Crear entorno virtual (opcional pero recomendado)

```bash
python -m venv .venv
.venv\Scripts\activate   # En Windows
# source .venv/bin/activate   # En macOS o Linux
```

---

### 3Ô∏è‚É£ Instalar dependencias

```bash
pip install langchain openai pinecone-client langchain-community langchain-text-splitters beautifulsoup4
```

---

### 4Ô∏è‚É£ Configurar variables de entorno

Creamos un archivo `.env` en la ra√≠z del proyecto con tus claves:

```bash
OPENAI_API_KEY=""
PINECONE_API_KEY=""
PINECONE_ENVIRONMENT=""
```


---

## üöÄ Ejecuci√≥n del Proyecto

Tenemos dos opciones:

1. El proyecto se ejecuta directamente en el notebook `RAG-OPENAI-PINECONE-ipynb`.

### Pasos dentro del notebook:

1. **Instalar dependencias** (primera celda).
2. **Configurar las API keys** (segunda celda).
3. **Cargar el documento** desde una URL (ejemplo: blog de Lilian Weng).
4. **Generar embeddings** e indexarlos en Pinecone.
5. **Consultar el sistema** con una pregunta en lenguaje natural.

---
2. Podemos ejecutar el proyecto con los archivos .py, para esto seguiremos los siguientes comandos:

```bash
python src/app.py
```
Esto desde la raiz del proyecto.

---

## üß™ Ejemplo de Ejecuci√≥n

1. Para el proyecto ejecutando desde consola app.py:

**Consulta:**

<img width="405" height="40" alt="image" src="https://github.com/user-attachments/assets/39ffcd7a-3629-4a96-8468-76b2cb96769f" />


```text
Que es la ia?
```

**Respuesta generada:**

<img width="836" height="74" alt="image" src="https://github.com/user-attachments/assets/c99b6c44-7c8b-4a09-a75f-e139206e1c63" />

```text
La inteligencia artificial (IA) consiste en ense√±ar a las m√°quinas a realizar tareas que normalmente requieren inteligencia humana, como aprender, adaptarse y crear. Esto incluye comprender el lenguaje, analizar datos y generar sugerencias √∫tiles. La IA combina diversas disciplinas, como inform√°tica, an√°lisis de datos, estad√≠stica, neurociencia, ling√º√≠stica y filosof√≠a. Un ejemplo de IA es el reconocimiento √≥ptico de caracteres (OCR), que convierte texto en im√°genes en datos estructurados, facilitando la obtenci√≥n de informaci√≥n valiosa.
```
2. Para el notebook:

**Consulta:**

<img width="495" height="111" alt="image" src="https://github.com/user-attachments/assets/d52fdf68-5ede-44ef-8fde-02c80fbee5e7" />


```text  
What is task decomposition?
```

**Respuesta generada:**

<img width="523" height="367" alt="image" src="https://github.com/user-attachments/assets/e0a67015-2a6a-4285-9fc7-9f030bdcfc92" />


```
--- Respuesta del agente RAG ---

Task decomposition is the process of breaking down a larger task into smaller, more manageable sub-tasks or goals. This can be achieved in several ways:

1. Using large language models (LLMs) with simple prompts, such as asking for "Steps for XYZ" or "What are the subgoals for achieving XYZ?"
2. Employing task-specific instructions, like "Write a story outline" for writing a novel.
3. Incorporating human inputs to guide the decomposition process.

Additionally, there is a distinct approach called LLM+P, which involves using an external classical planner for long-horizon planning. This method utilizes the Planning Domain Definition Language (PDDL) to describe the planning problem, where the LLM translates the problem into PDDL, requests a classical planner to generate a plan, and then translates that plan back into natural language.
Si quieres probar ask() ahora, descomenta la llamada o ejecuta ask("tu pregunta").             
```

---

## üñºÔ∏è Captura de Ejemplo y explicaci√≥n

Al ejecutar nuestro archivo app.py, podemos ver que se cargan los documentos de referencia, o bien el contenido de cualquier pagina web que le proporcionemos a nuestro programa, en el caos de cargar un docuemento, en este caso Referencias.txt podremos ver la siguiente salida en consola cuando el documento es procesado:

<img width="630" height="151" alt="image" src="https://github.com/user-attachments/assets/ec15a097-04c2-4dec-bc86-14bf0c435960" />


En esta ejecuci√≥n, el sistema carg√≥ un archivo local desde el equipo y lo proces√≥ como fuente de conocimiento. El pipeline de RAG ley√≥ el documento, lo dividi√≥ en 10 fragmentos (chunks) y posteriormente gener√≥ los embeddings correspondientes para almacenarlos en la base vectorial Pinecone. Este resultado demuestra que el flujo completo ‚Äîdesde la carga hasta la indexaci√≥n‚Äî funciona correctamente con documentos locales de tama√±o reducido.

Mientras que cuando se carga una pagina web como por ejemplo https://lilianweng.github.io/posts/2023-06-23-agent/ vemos lo siguiente:

<img width="625" height="152" alt="image" src="https://github.com/user-attachments/assets/9ae8bc66-9d4d-4ba2-b417-c1b9781369cd" />


En este caso, el sistema utiliz√≥ el cargador web para extraer el contenido de una p√°gina en l√≠nea, proces√°ndolo de la misma forma que un documento local. Debido a que la p√°gina conten√≠a m√°s informaci√≥n, el texto se dividi√≥ en 63 fragmentos, los cuales fueron convertidos en vectores e indexados en Pinecone. Esta ejecuci√≥n evidencia que el agente RAG puede integrar y comprender fuentes de datos externas, permitiendo consultas sobre informaci√≥n proveniente directamente de la web.

En los dos casos, el rag funciona correctamente y podremos realizarle preguntas como las siguientes:

<img width="1044" height="90" alt="image" src="https://github.com/user-attachments/assets/16bdbea1-6ffd-48e1-89a3-bac6ec0d05eb" />


<img width="1045" height="274" alt="image" src="https://github.com/user-attachments/assets/7557716a-1cf9-4cef-8c19-8735adbb4aa7" />


<img width="1048" height="340" alt="image" src="https://github.com/user-attachments/assets/82456591-4003-4e22-83f9-e7c043604e05" />


En este caso el agente respondio teniendo en cuenta la pagina web proporcionada de referencia, como podemos ver, funciona de forma correcta. 

---

## üë§ Autor

**Camilo Andr√©s Quintero Rodr√≠guez**
Proyecto: *Creaci√≥n de un agente RAG con LangChain, OpenAI y Pinecone*
Escuela Colombiana de Ingenier√≠a Julio Garavito ‚Äì 2025

